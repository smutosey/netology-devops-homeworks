# Домашнее задание к занятию «Микросервисы: подходы»

Вы работаете в крупной компании, которая строит систему на основе микросервисной архитектуры.
Вам как DevOps-специалисту необходимо выдвинуть предложение по организации инфраструктуры для разработки и эксплуатации.


## Задача 1: Обеспечить разработку

> Предложите решение для обеспечения процесса разработки: хранение исходного кода, непрерывная интеграция и непрерывная поставка. 
> Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.
> 
> Решение должно соответствовать следующим требованиям:
> - облачная система;
> - система контроля версий Git;
> - репозиторий на каждый сервис;
> - запуск сборки по событию из системы контроля версий;
> - запуск сборки по кнопке с указанием параметров;
> - возможность привязать настройки к каждой сборке;
> - возможность создания шаблонов для различных конфигураций сборок;
> - возможность безопасного хранения секретных данных (пароли, ключи доступа);
> - несколько конфигураций для сборки из одного репозитория;
> - кастомные шаги при сборке;
> - собственные докер-образы для сборки проектов;
> - возможность развернуть агентов сборки на собственных серверах;
> - возможность параллельного запуска нескольких сборок;
> - возможность параллельного запуска тестов.
> 
> Обоснуйте свой выбор.

### Ответ:

1) Базовое решение: **GitLab**, позволяет покрыть все требования. Для обеспечения требований достаточно Community Edition (CE), но Enterprise будет предпочтительнее для больших команд разработки и сложных продуктов.  
По требованиям:
   - cloud-native изначально. В Yandex Cloud есть GitLab Managed Service, можно также развернуть cloud-native версию (микросервисную) в своем k8s кластере через Helm-chart или Operator, либо же выделить Linux-сервер под Omnibus version;  
   - основывается на Git и в добавок для управления хранилищем использует Gitaly;
   - нет ограничения по репозиториям;
   - есть полноценный GitLab CI/CD, гибкая настройка запуска пайплайнов по событию в git (commit, MR), по расписанию, вручную с параметрами и downstream pipelines по событию в другом проекте и т.д.;
   - параметры можно задавать на проекте, группе или в общей системе, в остальном параметры и порядок (шаблон) сборки регулируются в пайплайне. Пайплайн обычно находится отдельно от основного проекта;
   - есть возможность в параметрах проекта или группы проектов хранить секреты, которые при запуске джобов будут скрыты, но также можно использовать внешнее хранилище секретов Vault и настроить взаимодействие раннеров с ним по JWT;
   - несколько конфигураций для сборки из одного репозитория возможно реализовать, настроив логику в пайплайне (например, в разрезе Deployments). Все шаги (джобы) полностью кастомизируются;
   - GitLab Runners могут использовать docker engine для выполнения джобов пайплайна, и джобы пишутся скриптовыми языками, что почти безгранично расширяет возможности. Можно использовать любой набор инструментов и кастомизировать сборку;
   - GitLab Runners могут располагаться как в облаке, так и на собственных серверах;
   - т.к. раннеры не ограничены в количестве, и каждый из них имеет concurrency режим, то все джобы могут выполняться параллельно, нужно лишь обеспечить необходимые ресурсы;
   - есть свои хранилища Package Registry (pypi, maven, helm и т.д.), а также Container Registry для хранения собранных образов Docker.  Каждая сборка позволяет сохранять артефакты пайплайна.


2) Продвинутое решение:
    - GitLab - как система контроля версий, управление проектами, репозитории, CI/CD
    - Hashicorp Vault - для хранения секретов
    - Sonatype Nexus - для хранения артефактов 
    - Также можно вынести CI/CD из Gitlab и использовать для этого Jenkins - в случае, если для кастомизации и конфигурации сборок не хватило возможностей GitLab. 



---

## Задача 2: Логи

> Предложите решение для обеспечения сбора и анализа логов сервисов в микросервисной архитектуре.
> Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.
> 
> Решение должно соответствовать следующим требованиям:
> - сбор логов в центральное хранилище со всех хостов, обслуживающих систему;
> - минимальные требования к приложениям, сбор логов из stdout;
> - гарантированная доставка логов до центрального хранилища;
> - обеспечение поиска и фильтрации по записям логов;
> - обеспечение пользовательского интерфейса с возможностью предоставления доступа разработчикам для поиска по записям логов;
> - возможность дать ссылку на сохранённый поиск по записям логов.
> 
> Обоснуйте свой выбор.

### Ответ:

Наверное, самым популярным решением будет стек ELK: **Elasticsearch** для хранения, **Logstash** для сбора логов и **Kibana** для визуализации и анализа.

Но я бы предложил использовать **Grafana-stack**, а именно:
- Сбор логов: агент **Promtail**, можно установить в k8s через Helm или как DaemonSet. Собранные данные пушит далее
- Обработка и индексация логов: **Grafana Loki**
- Хранение данных: **S3**-совместимое объектное хранилище (S3 managed в Yandex Cloud, MinIO, Ceph и т.д.)
- Визуализация и анализ: **Grafana**

Почему:
- cloud-native, состоит из микросервисов, легко и гибко масштабируется;
- push-система: мы не потеряем данные, если Grafana Loki какое-то время будет недоступен, пока Promtail их хранит и не отбросит по timeout.
- менее ресурсозатратный, т.к. индексирует только метаданные и написан на Go, а не Java. Подойдет для хранения больших объемов данных, но будет относительно медленнее обрабатывать объемные запросы на выборку.
- можно хранить данные во внешнем объектном S3-хранилище, сам Grafana Loki будет отвечать только за. Это будет плюсом для реализации в облаке;
- Promtail может сам обнаруживать таргеты (по принципу Prometheus) и гибко настраивается для сбора данных;
- Grafana предоставляет очень широкие возможности визуализации, анализа и даже алертинга;

Также можно использовать в качестве агента **Grafana Agent** - относительно новая разработка, но уже считается production ready. 
Этот агент позволяет собирать все типы данных для observability: логи, метрики и трейсы. 
Если делать реализацию на Grafana stack, то удобно использовать одного агента для сбора и отправки данных в Grafana Loki (логи), Mimir (метрики) и Tempo (трейсы), которые будут индексировать и хранить данные в одном S3-хранилище в разных buckets.


---

## Задача 3: Мониторинг

> Предложите решение для обеспечения сбора и анализа состояния хостов и сервисов в микросервисной архитектуре.
> Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.
> 
> Решение должно соответствовать следующим требованиям:
> - сбор метрик со всех хостов, обслуживающих систему;
> - сбор метрик состояния ресурсов хостов: _CPU, RAM, HDD, Network;_
> - сбор метрик потребляемых ресурсов для каждого сервиса: CPU, RAM, HDD, Network;
> - сбор метрик, специфичных для каждого сервиса;
> - пользовательский интерфейс с возможностью делать запросы и агрегировать информацию;
> - пользовательский интерфейс с возможностью настраивать различные панели для отслеживания состояния системы.
> 
> Обоснуйте свой выбор.

### Ответ:

Популярным решением будет Prometheus + VictoriaMetrics для долговременного хранения метрик + Grafana для визуализации. И в целом все решения будут основаны на визуализации Grafana и покрывать основные требования. 
Но, на мой взгляд, есть варианты оптимальнее:

1. Стек **VictoriaMetrics + Grafana**
   - Victoria Metrics уже развились до полноценной замены prometheus, их решение более эффективное и масштабируемое в cloud-native среде. Состоит из множества компонент
   - vmagent работает стабильнее Prometheus, т.к. функционал разделен с другими компонентами
   - очень эффективно хранит большие объемы данных

2. Продолжение реализации **Grafana-стека: Grafana Agent + Grafana Mimir + S3 + Grafana**
   - Grafana Agent выполняет target discovery, собирает нужные метрики и делает push в Grafana Mimir
   - Grafana Mimir позиционируется тоже как долгосрочное хранилище (вероятно, менее эффективное, чем VM), аналогично Loki индексирует данные и позволяет их хранить во внешнем S3-хранилище;
   - Grafana позволяет получать данные из источника Mimir, визуализировать их и строить дашборды

   Плюсы: 
   - Mimir имеет схожую структуру с Loki, состоит из компонентов-микросервисов и легко масштабируется;
   - Используем одного агента Grafana Agent для сбора всех данных, агент устанавливается оператором в k8s и сам выполняет target discovery;
   - Grafana Agent для сбора метрик использует внутри урезанную логику Prometheus и конфигурируется по аналогии, что не будет вызывать трудностей.
   - Grafana поддерживает различные источники данных, Mimir является Prometheus-like источником.
   В рамках одной архитектуры будет выгодно использовать идентичные по структуре и принципам решения;

---
